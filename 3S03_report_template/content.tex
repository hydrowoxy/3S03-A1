\section{Testing with generative AI}
\subsection{a)} 

I asked ChatGPT to specify three types of tests for the game described above.

The specific prompt I used was:

\begin{adjustwidth}{2em}{0pt}
{
Suppose I am developing an action-adventure game such that:
a) players explore a post-apocalyptic version of McMaster University; b) up to eight players 
may be accosted by zombies, mutated professors, angry Deans, killer squirrels, and more; c) 
gameplay can be both competitive and cooperative; and d) the game will be released on the 
PS5, Xbox Series X, Nintendo Switch and Sega Genesis (which is 16-bit and hasn't been 
manufactured since 1997).

Specify three types of tests for such a game.
}
\end{adjustwidth}

The response I received was as follows (it originally included more emojis and markdown formatting,
which I removed (and re-formatted) so that my document would compile and be legible):

\begin{adjustwidth}{2em}{0pt}
{

For a game like this — especially one that spans \textbf{modern consoles and a 16-bit relic like the Sega Genesis} — testing needs to cover functionality, performance, and platform constraints very carefully.

Here are \textbf{three distinct and appropriate types of testing}:

1. Functional Testing (Gameplay \& Mechanics)

\textbf{Purpose:} Ensure the game works according to design.

This would verify:

\begin{itemize}
    \item Players can explore the post-apocalyptic McMaster campus without falling through geometry
    \item Zombies, mutated professors, angry Deans, and killer squirrels spawn correctly
    \item Combat, health, inventory, and abilities function as intended
    \item Competitive and cooperative modes behave correctly
    \item Up to eight players connect and interact properly
\end{itemize}

Examples of functional test cases:

\begin{itemize}
    \item If 8 players join a session, all characters spawn correctly.
    \item If a Dean is defeated, loot drops trigger.
    \item Friendly fire behaves correctly depending on mode (co-op vs competitive).
    \item Save/load restores correct state.
\end{itemize}

This ensures the \textbf{core gameplay logic works}.

2. Performance \& Stress Testing (Multiplayer \& AI Load)

\textbf{Purpose:} Ensure the game runs smoothly under load.

Your game supports:

\begin{itemize}
    \item Up to 8 players
    \item Multiple enemy types
    \item Large explorable campus
    \item Both modern and legacy hardware
\end{itemize}

Testing would include:

\begin{itemize}
    \item Frame rate stability (60 FPS on PS5/Xbox, acceptable performance on Switch)
    \item Mass zombie swarm scenarios
    \item Network latency under 8-player online sessions
    \item Memory usage limits
\end{itemize}

Especially important:

\begin{itemize}
    \item The \textbf{Nintendo Switch} has weaker hardware.
    \item The \textbf{Sega Genesis} has extreme memory and CPU constraints (16-bit, limited RAM, cartridge storage).
\end{itemize}

Stress testing would reveal:

\begin{itemize}
    \item Frame drops when 50 squirrels attack simultaneously
    \item Network desync in competitive mode
    \item AI pathfinding breakdowns under heavy load
\end{itemize}

3. Compatibility \& Platform Testing (Cross-Console + 16-bit Constraints)

\textbf{Purpose:} Ensure the game works correctly across very different hardware environments.

You are releasing on:

\begin{itemize}
    \item PlayStation 5
    \item Xbox Series X
    \item Nintendo Switch
    \item Sega Genesis
\end{itemize}

Key concerns:

Modern Consoles (PS5 / Xbox Series X)

\begin{itemize}
    \item 4K rendering compatibility
    \item Ray tracing support (if implemented)
    \item SSD load speed optimization
    \item Controller haptics
\end{itemize}

Nintendo Switch

\begin{itemize}
    \item Lower resolution scaling
    \item Docked vs handheld mode behavior
    \item Limited GPU power
\end{itemize}

Sega Genesis

\begin{itemize}
    \item 16-bit graphics limitations
    \item Severe RAM restrictions
    \item Cartridge size constraints
    \item No modern multiplayer networking
    \item Likely redesigned gameplay mechanics
\end{itemize}

Compatibility testing ensures:

\begin{itemize}
    \item UI scaling works at different resolutions
    \item Input mapping works on each controller
    \item Save systems comply with platform standards
    \item The Genesis version doesn’t exceed hardware limits
\end{itemize}

Summary

The three appropriate testing types are:

\begin{enumerate}
    \item \textbf{Functional Testing} – Does the game behave correctly?
    \item \textbf{Performance \& Stress Testing} – Does it run smoothly under load?
    \item \textbf{Compatibility/Platform Testing} – Does it work properly across PS5, Xbox, Switch, and Genesis?
\end{enumerate}

Given the extreme hardware diversity (especially including a 1990s 16-bit console), 
\textbf{platform compatibility testing would be especially critical} for this project.

}
\end{adjustwidth}

\subsection{b)} Two poor, incorrect, or unclear aspects of the response are:

\begin{enumerate}
\item Tests are categorized by type (functional, performance, compatibility) 
but not granularity (unit, integration, system).

We have seen in class that unit and integration testing are often conflated. This is
bad because the distinction affects feedback speed, debugging difficulty, and test design. 
The AI response not only fails to make this distinction, but also fails to even mention 
unit testing; it is unclear whether the tests it suggests are to be implemented as unit, 
integration or system tests (though it seems the AI is relying only on system-level 
tests based on the descriptions provided). This is a significant omission, as unit 
tests are crucial for verifying the correctness of individual components in isolation. 

This is unhelpful because it does not indicate how the test suite should be structured 
in practice or how trade-offs between speed, isolation, and scope should be managed. We
have no idea how to best implement these tests in practice.

\item No measurable adequacy criteria are provided for any of the tests.

Adequacy criteria such as statement, branch, or MC/DC coverage are completely omitted. 
Coverage is defined as a way to measure a proportion of the structure or domain that a 
program, test case, or test suite exercises: this helps us understand when testing is
sufficient. The AI response provides no guidance on how to determine when testing is
adequate. 

This is unhelpful because it provides no guidance on how to organize the tests for 
fast feedback and effective fault isolation. We have no idea what makes tests 
"good enough" using the AI's suggestions, and thus no way to know when to stop 
testing or how to prioritize test cases.

\end{enumerate}

\subsection{c)} Two good, valid, or helpful aspects of the response are:

\begin{enumerate}
\item It recognizes system-level risks.

The response lists performance issues, hardware constraints, and
multiplayer load as potential failure sources. These issues typically
emerge when multiple components interact or when the system is exercised
in realistic operating conditions, rather than at the level of isolated
units.

This is helpful because it acknowledges that some failures only become
visible when the system is evaluated as a whole. We can use this to identify 
system testing needs.

\item Concrete examples of test cases are provided for each type of testing.

The response includes specific example scenarios (eight players joining a
session, friendly fire toggling, stress-testing large enemy swarms).
Clear and expressive tests are valuable because they make expected behavior
explicit and easier to reason about. By describing concrete scenarios rather
than only abstract test categories, the response clarifies the intended
behaviors that should be exercised.

This is helpful because clearer behavioral intent makes it easier to design
tests that meaningfully verify system behavior. We can more directly translate
these scenarios into implementable test cases with well-defined expectations.

\end{enumerate}

\subsection{d)} Considering the above, three tests I would suggest are:

\begin{enumerate}
    \item Given the combat damage logic with game mode set to cooperative,
    verify that when Player A attacks Player B, Player B does not take damage; 
    and when the session is switched to competitive mode, Player B does
    take damage (covering both decision outcomes).

    \item Simulate two players in the same multiplayer session using a
    test double (stub/fake network transport object that simulates real network 
    communication). When Player A defeats a zombie, verify that Player B's 
    client updates to reflect the zombie's removal.

    \item On each target platform build, run the following smoke test:
    launch $\rightarrow$ start/join session $\rightarrow$ combat event
    $\rightarrow$ exit session, and verify that no crashes or state
    inconsistencies occur.

\end{enumerate}

I prefer these tests because they explicitly cover unit, integration, and system
granularity while including a measurable adequacy goal (exercising both decision outcomes)
for core logic. This results in fast, deterministic tests for core gameplay defects 
while keeping broad smoke tests to catch platform-specific and full-system 
failures.

\section{Testing with Junit}

\subsection{Program 1}
\subsubsection{a)}
The loop starts at the last index of the array and decrements \texttt{i}, continuing as long as \texttt{i>0}.
This means it will terminate when \texttt{i=0}. This is problematic because it means you will never check
index 0.

Using the given test as an example:

\begin{adjustwidth}{2em}{0pt}{

    We pass \texttt{x = [2,3,5]} looking for \texttt{y = 2}. 

    \texttt{x.length = 3} so \texttt{x.length - 1 = 2}, so the loop starts with \texttt{i = 2 > 0} 
    and checks if \texttt{x[2] == y}. 

    \texttt{x[2] == 5 != 2}, so this is false.

    Then it decrements to \texttt{i = 1 > 0} and checks if \texttt{x[1] == y}. 

    \texttt{x[1] == 3 != 2}, so this is false.

    Then it decrements to \texttt{i = 0}. \texttt{0 > 0} is false, so the loop terminates without 
    checking if \texttt{x[0] == y} (which we know is true).

    The program defaults to the sentinel value of \texttt{-1}, which is returned when the loop
    terminates. The program incorrectly returns \texttt{-1} even though the target value is present 
    in the array.

}\end{adjustwidth}

Fault:

\begin{adjustwidth}{2em}{0pt}{

    The fault is in the loop termination condition \texttt{i > 0}, 
    which never checks index \texttt{0}.

}\end{adjustwidth}

Proposed modification to the code:

\begin{adjustwidth}{2em}{0pt}{

    Change the loop condition to \texttt{i >= 0} so that index \texttt{0} is also checked.

}\end{adjustwidth}

\subsubsection{b)}
A test case that does not execute the fault is:
\begin{adjustwidth}{2em}{0pt}{

    \texttt{x = null; y = anything; Expected = NullPointerException}.   

}\end{adjustwidth}

Evaluating \texttt{x.length} in the loop initialization throws a 
\texttt{NullPointerException} before the faulty loop condition \texttt{i > 0} 
is evaluated; the faulty condition is never executed.

\subsubsection{c)}
fewfw
\subsubsection{d)}
fewfe
\subsubsection{e)}
werrg

\subsection{Program 2}
\subsubsection{a)}
\subsubsection{b)}
\subsubsection{c)}
\subsubsection{d)}
\subsubsection{e)}

\subsection{Program 3}
\subsubsection{a)}
\subsubsection{b)}
\subsubsection{c)}
\subsubsection{d)}
\subsubsection{e)}

\subsection{Program 4}
\subsubsection{a)}
\subsubsection{b)}
\subsubsection{c)}
\subsubsection{d)}
\subsubsection{e)}

\section{Testing parts of large systems}

\section{Test driven development (TDD)}

\section{Test coverage and AI}